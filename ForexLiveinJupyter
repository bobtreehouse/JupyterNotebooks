
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Jan  7 18:30:25 2019\n",
    "\n",
    "@author: bobtr\n",
    "\"\"\"\n",
    "\n",
    "import bs4 as bs\n",
    "import urllib.request \n",
    "\n",
    "save_path = 'C:/Users/../Clocks/'\n",
    "\n",
    "\n",
    "sauce = urllib.request.urlopen('https://www.wsj.com/').read()\n",
    "soup = bs.BeautifulSoup(sauce, 'lxml')\n",
    "\n",
    "nav = soup.nav\n",
    "body = soup.body\n",
    "\n",
    "#print(nav)(remove hash in front of this line to print the entire nav content)\n",
    "\n",
    "for paragraph in body.find_all('h3'):\n",
    "    print(paragraph.text)\n",
    "    \n",
    "#for paragraph in body.find_all('a.wsj-headline-link'):\n",
    " #   print(paragraph.text)\n",
    "    \n",
    "#for paragraph in body.find_all(\"div\"):\n",
    " #   print(paragraph.text)\n",
    "    \n",
    "filename = 'wsjnews.txt'\n",
    "f = open(filename, \"w\") \n",
    "f.write('')\n",
    "f.close()\n"

   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Jan  7 18:30:25 2019\n",
    "\n",
    "@author: bobtr\n",
    "\"\"\"\n",
    "\n",
    "import bs4 as bs\n",
    "import urllib.request \n",
    "\n",
    "save_path = 'C:/Users/.../PythonDailyNews'\n",
    "\n",
    "\n",
    "sauce = urllib.request.urlopen('https://www.forexlive.com/').read()\n",
    "soup = bs.BeautifulSoup(sauce, 'lxml')\n",
    "\n",
    "nav = soup.nav    \n",
    "body = soup.body\n",
    "\n",
    "for paragraph in body.find_all(\"div\",{\"class\":\"teaser-text\"}):\n",
    "    print(paragraph.text)\n",
    "    \n",
    "filename = 'forexdailynews.txt'\n",
    "f = open(filename, \"a\") \n",
    "\n",
    "f.write('')\n",
    "f.close()\n"


